# s 
56
## url
https://towardsdatascience.com/data-mesh-solution-and-accelerator-patterns-acffbf6e350
## tiny url
https://tinyurl.com/data-as-a-product-s56
## archive url
https://bit.ly/data-as-a-product-s56
## title
Data Mesh Solution and Accelerator Patterns
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE**: source and references
``` python
s56 = CClass(source, "s56", values={
    "title": "Data Mesh Solution and Accelerator Patterns",
    "url": "https://towardsdatascience.com/data-mesh-solution-and-accelerator-patterns-acffbf6e350",
    "archive url": "https://bit.ly/data-as-a-product-s56",
    "tiny url": "https://tinyurl.com/data-as-a-product-s56",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
``` 

> Data Mesh Solution and Accelerator Patterns

> Far too often we are preoccupied by Data Mesh technology while ignoring the incredible business outcomes it can deliver. Here are some of the best opportunities for Data Mesh in the enterprise.

![Unsplash](https://miro.medium.com/max/786/1*0QYCZWirY4dityRQYXNG4A.webp)

> Delivering Outstanding Business Outcomes with Data Mesh
Like many of you, I am a technologist at heart, and I absolutely love to deep dive into low level Data Mesh technology. But my experience has shown it is just as important to understand the benefits of Data Mesh as its implementation details.
Why? Well, I suppose, it is a practical reality that any Data Mesh journey will not be a trivial endeavour and will probably cost millions of dollars. Nevertheless, someone must pick up the tab. And to address this we should be prepared to answer a very practical question: What business outcomes can be achieved by Data Mesh?
In this article series, I will introduce several “solution” and “accelerator” patterns that combine Data Products and Data Mesh capabilities to deliver practical and tangible business outcomes. Subsequent articles in this series will explore in detail each of the solution and accelerator patterns.
This article assumes that you have a high-level understanding of Data Mesh. If you need some background information on Data Mesh, there are a number of great articles are available here (patterns), here (architecture), here (principles) and here (lessons learned).

> Data Mesh Patterns Apply to Operational & Engagement Systems (not just Analytics)
Data Mesh binds analytics systems and operational / engagement systems through a common set of “plumbing”, organization constructs (ownership, platform-thinking), and governance concerns. As Zhamak Dehghani points out in her recent book “data mesh suggests closing the gap and feedback loop between the two [analytic and operational] planes, through data shared as a product and oriented around the domains. Data mesh connects the two planes [analytic and operational] under a new structure — a network of peer-to-peer connected data products and applications.”
But I will go a bit further. I suggest that the Data Mesh principles — data ownership, data as a product, self-serve, and federated governance — can be applied equally well to systems of insight as well as systems of record and engagement. In fact, in addition to “analytics data products”, I advocate for “operational data products” and “engagement data products” that bring the power of Data Mesh to systems/data of record that run an enterprise and those that engage our customers.
Now, to be clear I agree with Dehghani’s point that Data Mesh is currently focussed on analytics because that is where the pain is today, but that does not change the fact that Data Mesh principles and patterns can benefit a broad array of challenges across the enterprise, including those related to systems/data of record of systems/data of engagement.
So, as you progress with this article you will see Data Mesh solution and accelerator patterns that span not only analytics data products but also operational and engagement data products.

![DifferentDataProducts](https://miro.medium.com/max/786/1*0Ppn14HWTqiplC3pzZkrYg.webp)

> Solution, Accelerator, and Foundational Patterns
There are several groups of patterns that are composed to deliver outstanding business outcomes:
Foundational Patterns include low level patterns such as Change Data Capture and Event Streaming BackbonePatterns (a full list is provided later in this article). These patterns are typically used to build the core enterprise Data Mesh but also may be used directly to assemble a solution pattern. These patterns were discussed in detail in previous articles.
Accelerator Patterns provide high-level building blocks that are composed to address point elements of a business problem. In many cases they are assembled from foundational patterns.
Solution Patterns are a combination of foundational and accelerator patterns that address a practical business problem.

![DifferentPatterns](https://miro.medium.com/max/786/1*7o5srkXWombOLz2IFI9lLA.webp)

> Solution Patterns
So, just as data across the enterprise is interconnected, solution patterns try to address challenges in connecting systems of insight, engagement, and record. Hence, there is no practical limit other than imagination to creating and deploying solution patterns.
As you read through these patterns you will probably recognize that there are many ways to address the business challenges that they address. These solution and accelerator patterns have been selected since Data Mesh, with its core principles and complementary architecture, is particularly well suited to address these challenges in a practical manner.
So, let’s dive in!
Below are a few of the most common solution patterns:

![SolutionPatterns](https://miro.medium.com/max/1400/1*y4ZrthjbDtKKjYbhUCI6_Q.webp)

> AI/ML Enablement Data Mesh solution pattern (Figure 3) shows how Data Mesh can establish a real-time analytics environment that supports data lineage. Data Mesh uses the “Live Replica” accelerator pattern (discussed later in this article) to create a data product that is a near real-time copy of data from operational systems. But just as important, this pattern uses Change Data Capture, Event Streaming Backbone, and Immutable Audit/Change Log foundational patterns to provide data lineage required for model reproducibility and traceability required in regulated industries such as Financial Services and Health Care. (Deep-dive article coming soon)

**OPEN CODING TRACE:**

Recognised options: ai_ml_enablement_pattern, change_data_capture, event_streaming, immutable_change_audit_log, real_time_data_access

**AXIAL CODING TRACE:**
added:
``` python
    ai_ml_enablement_pattern
``` 
add codes to s57: 
``` python 
    '''
        change_data_capture,
        event_streaming,
        immutable_change_audit_log,
        real_time_data_access
    ''' 
```

> System of Engagement Data Mesh solution pattern (Figure 3) accelerates the implementation of “systems of engagement”. Where Data Mesh commonly addresses “systems of insight” (a.k.a. analytics systems), Data Mesh patterns can be applied just as easily to establish “systems of engagement”. These systems of engagement combine information from multiple sources to provide a consistent and holistic views of customer data and all related data necessary to deliver an outstanding customer experience. Data Mesh uses the Live Replica accelerator pattern (discussed later in this article) to move data in near real-time from operational systems (or data products) into engagement data products. (Deep-dive article coming soon)

![CloudAccelaration](https://miro.medium.com/max/786/1*2UPoMZFFRnRmGTBuuMfe5A.webp)

> Legacy Modernization Data Mesh solution pattern (Figure 4) uses the Strangler and CQRS accelerator patterns (discussed later in this article) to migrate data in a safe, reliable — and unobtrusive — way. And as data is migrated in near real-time to the new platforms a consistent new data product is made available which can substitute for, and over time, replace older legacy platforms. (Deep-dive article coming soon)

> Cloud Acceleration Data Mesh solution pattern (Figure 4) uses the Live Replica and Strangle accelerator patterns (discussed later in this article) to dramatically accelerate your cloud migration. This pattern moves data from any type of systems (transactional or analytical) or data product to a secure cloud resident data store. It is fundamental to creating the data “gravity well” that is required to cost effectively build high performance cloud-native systems. (Deep-dive article coming soon)

![MDMPattern](https://miro.medium.com/max/1400/1*v7xJvshw631PRHt0DbjCIQ.webp)

> Zero Trust Data Mesh solution pattern (Figure 5) creates a secure and reliable data product. All enterprises — especially those in regulated industries — manage sensitive data (regulatory examples include GDPR, PCI, PII, HIPPA). Yet safeguarding that data is expensive, time consume, and hard to get right. And I am pretty sure too many projects to count that have been delayed by the lack of foresight to anticipate security needs. This solution pattern addresses this issue by creating repeatable process (DevSecOps) to quickly spin up a secure mission-critical — zero-trust — Data Product. (Deep-dive article coming soon)

> Master Data Management (MDM) Mesh solution pattern (Figure 5) helps to understand where your book-of-record, or master, data and relevant related data resides. Traditional MDM solutions make copies of master data that require complex data migration and synchronization techniques. On the other hand, this pattern uses the Enterprise Data Product Catalog to allows data to reside in their natural data products but uses the discoverability feature of data products to gather links to master data information dynamically. (Deep-dive article coming soon)

**OPEN CODING TRACE:**

Recognised options: zero_trust_architecture, mdm, discoverability

**AXIAL CODING TRACE:**

add codes to s57: 
``` python 
    '''
       add_force_relations({mdm: {centralization: positive,
                                   discoverability: positive}
                     })    
    ''' 
```

add codes to s57: 
``` python 
    '''
        zero_trust_architecture,
        mdm
    ''' 
```

> Accelerator Patterns
Accelerator patterns are high-level building blocks, frequently built upon foundational Data Mesh patterns, that can be combined to accelerate the delivery of solution patterns. Below are a some of the most common.

![AcceleratorPatterns](https://miro.medium.com/max/786/1*4cSM8BSHKJFJjuYvnqgozQ.webp)

> Data Product Observability accelerator pattern provides visibility into a Data Product (detailed article available here). This pattern surfaces several important static and dynamic data product attributes (accessible via the data product catalog/dashboard) including:

> Data Movement, providing low-level event tracing within a Data Product but also correlates events that occur both inside and between data products; This information is crucial to debugging and gain visibility into highly asynchronous communication within and between data products.
Meta-Data, providing information about the data managed within the Data Product.
Interfaces Definitions, providing visibility to access methods for a Data Product.
Events Schemas, providing definitions of events that are consumed or emitted by the Data Product.
Metrics, providing information about data product consumption patterns.
Data Lineage, providing lineage of data within the Data Product.
Data Access Rights, providing visibility into security and access rights required to access the data product.
Alerts and Errors, providing visibility into operational alerts and errors to support production monitoring.

**OPEN CODING TRACE:**

Recognised options: observation_plane, 
New options: debugging, clear_interface_definitions, data_lineage, consumption, observability

**AXIAL CODING TRACE:**

add codes to s57: 
``` python 
    '''
       debugging = CClass(force, "Debugging")
        
       add_force_relations({observation_plane: {debugging: positive,
                                                understandability: positive,
                                                data_lineage: positive,
                                                consumption: positive,
                                                observability: positive}
                     })    
    ''' 
```

add codes to s57: 
``` python 
    '''
        observation_plane,
    ''' 
```

> Data Product Security accelerator pattern provides a granular security model to protect data products (deep-dive article coming soon) .It implements policies to secure a data product including the following (note: I use a database/attribute/row nomenclature for simplicity, but these security constructs can be applied to unstructured or file-based data):
Role Based Access Control, that ensures that only entities that are authorized (ie. have been assigned an authorized role in their profile) can access the Data Product.
Attribute Based Access Control, that protects access to individual columns, fields, or attributes of data within the Data Product.
Row Based Access Control, that protects access to individual rows or groupings of related data within the Data Product.

**OPEN CODING TRACE:**

Recognised options: role_based_access_control, attribute_based_access_control

**AXIAL CODING TRACE:**

add codes to s57: 
``` python 
    '''
       role_based_access_control
    ''' 
```

add codes to s57: 
``` python 
    '''
        role_based_access_control,
        attribute_based_access_control
    ''' 
```

> Data Product Operability accelerator pattern forwards alerts and errors from a Data Product and emits them to interested parties, including for example an enterprise monitoring capability. Note there is potential overlap with the “Data Product Observability” pattern, but the difference is nuanced but nevertheless important: the Data Observability pattern captures alerts and errors inside a Data Product, the Data Product Operability pattern is designed to emit these errors (and perhaps aggregate them as needed) to interested parties. (detailed article available here).

> Strangler for Data Products accelerator pattern uses the well-known Strangler-Fig pattern is a practical and feasible mechanism of migrating data from one Data Product to another. Data Mesh uses several foundational patterns including APIs, Change Data Capture, and Event Streaming backbone to migrate data safely and reliably between data products. (Deep-dive article coming soon)

![CQRSPattern](https://miro.medium.com/max/786/1*jgaeUenqi475IHMehvI6WA.webp)

> Live Replica accelerator pattern creates a near real-time (latency of 50–500 milliseconds) live replica of data. Data Mesh uses the Change Data Capture and Event Streaming Backbone foundational patterns to move data safe, reliably, and in near real-time to a synchronized (to within 50–500 milliseconds) data product. (Deep-dive article coming soon)

> CQRS for Data Products accelerator pattern applies the well-known CQRS pattern to optimizes performance by segregating data product read and write activities between Data Products. Data Mesh uses APIs, Change Data Capture, and Event Streaming Backbone foundational patterns to gracefully and reliably segregate data access methods. (Deep-dive article coming soon)

**OPEN CODING TRACE:**

Recognised options: cqrs
New option: strangler_fig_pattern
New force: easy_data_migration_between_products

**AXIAL CODING TRACE:**

add codes to s57: 
``` python 
    '''
        strangler_fig_pattern = CClass(pattern, "Strangler-Fig Pattern") 
        cqrs
        
        easy_data_migration_between_products = CClass(force, "Easy Data Migration Between Products")
        
    ''' 
```

> Foundational Patterns
The lowest-level components — data mesh foundational patterns — are used to compose both accelerator and solution patterns. These are discussed in other articles:
Foundational Patterns Summary: Introduction to the patterns, how they work, and how they interact,
Data Mesh Architecture, that proposes an architecture that binds various Data Mesh patterns and components,
Change Data Capture, that captures and transmits data inside and between Data Products,
Immutable Log (Data Lineage), that captures data changes and aggregates them into data lineage records,
Event Streaming Backbone, that transmits events inside and between data products, as well as the broader enterprise,
APIs, that provide the contract for interfaces for a data product, and,
Data Product Catalog & Enterprise Data Product Catalog, that provide meta-data and definitions for schemas used by a Data Product.

![FoundationalPatterns](https://miro.medium.com/max/786/1*gSQMYdzZhkq4ghAkeabLDA.webp)

> Concluding Thoughts
An Enterprise Data Mesh is the emerging foundation of the real-time digital enterprise. Solution, Accelerator, and Foundational Patterns provide a practical way of understanding and addressing common business and technical challenges. And by using these patterns, Data Mesh provides a practical way of addressing these challenges to create real value and tangible business outcomes.
Hopefully this article gives you the necessary insight to kickstart your own Enterprise Data Mesh!

