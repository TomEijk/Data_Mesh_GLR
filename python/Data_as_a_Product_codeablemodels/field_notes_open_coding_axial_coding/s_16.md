# s 

## url

## tiny url

## archive url

## title

## source code

## example

## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE**: source and references
``` python
s = CClass(source, "s", values={
    "title": "",
    "url": "",
    "archive url": "",
    "author type": "",
    "type": "",
    "example": True,
    "source code": True})
r = CClass(reference, "", values={
    "bibliographic reference": "author_name: book_name}" + "publisher, year",
    "author type": "Practitioner",
    "type": "Practitioner Book"})
``` 

# coding

Data Mesh Radio is provided as a free community resource from DataStacks.
Welcome to Data Mesh Radio, produced and hosted by Scott Hurlman,
the founder of the Data Mesh Learning Community.
Data Mesh Radio is a vendor independent resource for learning more about Data Mesh.
Let's jump in!
In this episode, I interview how it will plot the CTO at Agile Labs,
a consulting company that's been working on Data Mesh implementations for almost two years now.
Apollo has a really interesting concept that he's been running with his clients that are looking at Data Mesh around how to find your data products.
And that's kind of a two-phase approach of how to find your data products that already exist that you need to migrate to the Data Mesh,
as well as how to figure out what data products you need to support the business processes that you should be supporting with data products by creating kind of consumer-aligned data products.
He gives us some great information about putting together an implementation plan for how you're actually going to go and create these data products or migrate them to the Mesh.
We talked somewhat about measuring the success of a data product.
Is that something that's still an emerging concept? I think the answer is yes, but he's got some interesting thoughts on where we are right now on how to measure that.
And I think one of the big focus points is talking about this information gap and why that's a good way to model out what you need to do to create data products in Data Mesh.
If you have an approach to actually talk with the business stakeholders and bring them in and not try to just say here's data to support what you're doing, but really dig down into what are you trying to accomplish and why and how could data support that and work backwards?
Rather than trying to create a data model and things for those consumers and just say here it is, use it.
And they're kind of like, oh, this is great and I'll know how to deal with this.
So this was a really awesome interview. I learned a lot from it and I think this is a really interesting approach to kind of going domain by domain once you've kind of mapped out your
text for figuring out again what data products you've already got in some semblance and where you need to really work on creating these things to really align the business line of business folks to what you're trying to do with data mesh.
I, as I said, really enjoyed this and I hope you do too.
Welcome everybody to this episode. I'm really excited to have Paulo Plather here from agile labs. He's been working on some data mesh implementations with customers and has some really great perspective.
So, Paulo, could you tell everybody, you know, kind of your, a little bit about your background and then we can jump into your background around data mesh specifically, and a little bit about what agile labs does as well.
Yeah, sure. It's a pleasure for me to be here. Thanks for inviting me. I'm Paulo Plather. I'm the CTO and co-founder of Agile Lab. Agile Lab is a consulting firm focusing on big data technologies and data management in general.
Our journey started in 2013, so we are really early adopters of such kind of paradigm.
And we love to craft data platform for our customer and try to push them to embrace new technologies and new paradigms to become more data driven.
And obviously, data mesh is one of those. My personal background, instead, I am a software engineer.
I work in finance for a while, and then when we started to, when we started agile lab, I completely changed my focus as we ching on data management and big data technologies.
And now agile lab is a company of something like 80 people working across multiple customer, multiple countries. So we are looking forward to drive our internationalization in the next years.
Awesome. Thank you very much. And I also have the financial services background. I'm finding that is pretty common around the data mesh space.
I don't know if it's everybody tried to run as far away from financial services or that it's just that those are the people who've seen the pain most acutely. I think it's for the second.
So, yeah, thanks so much for being on and, you know, can you give people a little bit of background about what you've been doing with data mesh and then we can get into kind of the topic that you want to cover today, which is data product flow.
Yeah, we started with data mesh in early 2020 with a customer was. So, it is more or less two years that we are digging into such concept.
We also experimented a lot of solutions and architecture and fail it also several times that is good that is good to learn something.
And we are still refining our vision related to data mesh. We are also building platforms for customer and also an internal platform that will become probably a product that is a data mesh enabler.
And right now we have several data mesh initiatives running with different customers so we are gaining a huge perspective on this topic with different kind of companies and organizations and one is more agile someone is more structured.
So, we are experimenting a lot also in terms of adoption curve and how to drive a roadmap of implementation in different kind of organization that is one of the most important topic for for data mesh because.
And then an architectural pattern is an organizational and social technical pattern.
So, it's really really a complex topic to address in the in the actual organizations.
Yeah, I'm finding this a lot too of people want you know kind of super structured guidance and a lot of it is well it depends and you know it's not just okay here's how you even do just like a data literacy program or here's how you do X or Y or Z it's it's very.
And necessary to think about what are your constraints and I like what you said of you know your building platforms for customers and then building also a tool to enable platforms because I think that if we try to have cookie cutter platforms for everybody it's it's going to fail right because the way people work and the the technologies that people want to use or capabilities that they need.
Our different and so if we can create the the capabilities to build that platform or you know talk about how we abstract capabilities away to give to the domains to make their jobs easier it's it's really interesting so.
You know you specifically wanted to cover data product flow and you give a little bit of background as to what that means to you and kind of where you want to dig into today and and you know kind of discuss that.
Basically what is happening in along these two years as that when when you're when you're approaching a new customer and explaining the data mesh the enables the different planes of data product experience a supervision plane okay everything is fine the concept is super cool.
But at certain point someone will ask you okay but how do I identify a data product or how do I identify all the data products that I have potentially in my in my domains know.
And this question is really is really is really hard to answer so.
We tried to to define a standard standard standard way or maybe it's better to say.
That is the procedure to drive this discovery process know the way you discover the data products that you have in the domain or that you still don't have in your domain.
So, because because that a mesh is fully inspired by the main driven design principles first first of all we tried to to understand how this problem is addressing in software software crafting when when you apply the main driven design principles to to the software design.
And we discovered that there are several ways to address this there are many many.
Practice to do that and the one that is part of the has the most was.
The reason why even storming is very good to apply in in the data landscape is because even storming is focusing on events now and events are the the foundation to create some immutable storage of data.
Also, if you want to address bit and plurality feature that is proposed by by Danak in in how data product should persist the data.
Basically, when you track all the events in your business process you are tracking all the possible data that will be generated because every event is generating some some data.
Instead, the other other procedures to discover domains are more focusing on communication flows, human interaction stuff like this that is not really.
Findable to to the data, okay, instead the events are really foundation for also for the analytical the analytical practice. So, it's almost like you're you're talking about sharing your business context via.
Not direct exchange of data and that that also is kind of creating a data product so how do you create that that people who don't necessarily have that one to one connection or to make it more scalable, how do you share that is is that the way is that what you're talking about or
still talking about how starting from the business process that is an underlying basically every kind of business. So, if we talk about the sales domain.
You will have several processes running inside the the sales domain. So, the first step of data product flow is to identify aggregates using the even storming procedure that is a way to track and discover your business process and rationalize it.
According with domain driven design principle and create the the aggregates okay but aggregates are still not data product or maybe aggregates can be muppet with the source align it that a product okay so all the data products that are really
pair them binded with the operational systems that is where the business process is running so the business process is running on top of operational systems operational systems are generating data and then you can map this data with source align data products.
Okay, the next step is okay but in the data analytics space I want to create new data I want to extract value from from my data and this is not strictly related with the business process itself.
The idea here is to instead still rely on on the business process so what we do so the first part is even storming to create such aggregates and then it start the data product flow procedure.
Basically what we do on each step of the business process we we try to discover which are the business decision that people are taking in order to to develop the business process.
Let's do an example if I am in a recruiting process in HR in in the HR domain let's say at certain point I need to screen the the the curriculum that are coming in my HR system okay.
In this in this phase there are people that are basically opening the curriculum looking at the characteristics of the candidate trying to match with the role and the position that they are offering so.
This is a business decision I mean so they have to decide which candidate is good or not to go to proceed in in the business process okay.
So once we identify all the business decision that domain expert are taking along the process we start from here and we try to understand okay but if I want to automate this decision or if I want to support this decision maybe it's not possible to fully automate a business decision coming from a domain expert but it's possible to help.
This and support this business decision to augment the efficiency to reduce cost to save a time from people and so.
And from here we start to draft and to design the consumer line at data product so okay we we understand that there is a business decision that we can automate and we try to link this business decision with all the data with all the events that we define it previously with the even sourcing step.
And in this way we we realize which data products are useful to improve our business process so it's very business oriented way to think to the data know because we are not modeling the data.
Starting from the data we are modeling the data starting from the business decision that are impacting the business process so it's really business oriented way to think to the data and we are going backward from business decision until to reach the data product.
So oriented data product that are on the on the operational side of the story.
Basically you're you're looking for the gaps with that or because if you're saying okay I'm the domain and here's the data that I know I have and it might be that people conceptualize here's the data that other people would want to use.
And that you're not sharing every piece of data you've got right it that can be obviously quite overwhelming.
But then you go to the other side and say okay what is the demand of what people are already doing and you know you could probably extend that to go and interview those business process people and say what's the demand of what you can't do that you would like to do and that that goes into your data product roadmap but you're you're saying.
What is it that you're actually doing where and then what's the data that could help to inform or drive this decision whether it's again the automation because it's.
Toil work and it's not something where humans really need to be involved versus it is something that humans should be involved with and that they're the ones but they need their decisions would be made in a more efficient manner whether that's.
Better decisions or quicker decisions or whatever if they have more data and then you kind of go back to the source and say how can we get this data or what data can we get to augment and make that decision that business process decision is is that a good summation of it yeah yeah.
Exactly it's a really interesting concept for for go because it's it's kind of supply side and demand side of data of going and doing kind of your economic analysis of that I love it.
And when we talk about consumer align that that's a product, this is the meaning now we are we are starting from the demand of data trying to extract all the domain knowledge that is employed to perform a business a business process.
And this is a really consumer oriented probably in.
Now now the Zamakay is changing a bit saying that is a fit for purpose data products know and then we are going backward because.
When you start from the business decision you realize that need that you need some data but maybe this data is not present in your data product map so you realize that you need to build another data product in order to fulfill.
There are requirements so we are going backward until we hit with the source oriented data product and we are doing this with a with a workshop so as a collaborative workshop with business stakeholders so.
Typically now in the pandemic we are doing everything with the mirror but obviously it's possible to do it also live with a whiteboard so.
It's a little sticky notes instead of just a little mirror board sticky notes but yeah also because live when you are live in a room is easier to get engagement from business people and so on.
But I can ensure that in the first phase so when you have just to map the business process maybe business stakeholders will be a bit annoyed because probably they have done this several times now okay I need to explain again the process from the start to the end.
When they start to talk about how to automate their job how to speed up their time consuming tasks and so on.
They are getting engaged and and the workshop is very also is also fun not because you can you can see you can wander whatever you want and you can start to draft how to reach the goal so.
I can see that you you say hey I'm not just here to try and shove some data in front of you I'm here to actually listen to you and serve what your needs are I understand that you've explained this before but we.
There wasn't necessarily a great bridging between what are your needs and then what are we going to do to serve that and and not just serve it once right I think this is the thing with data mesh is the repeat ability is your your you have an owner around this data.
If it's changing you're going to know about it if you're you know signing up to kind of know about changes and things like that where there's going to be care to this and it's not a one off where we're going to try and.
You're making more efficient by changing your process today instead of we're going to inform you and evolve with you as your needs change and that I think that's the empathy aspect and and I haven't heard.
I mean you're kind of talking about consuming domain rather than producing domain but I do think that this can get domains as well bought it right where you say hey domain we're going to want you to serve up your data but first let's talk about what your needs are and then we'll also help you get that data yourself.
You're kind of getting more and more people bought in as to this can be helpful because it's the it's kind of a reverse prisoner's dilemma of you know where you say hey yes if I don't do any work and I just get the the data am I the best off probably but if everybody does all of the work.
To share your data effectively so I like this this concept of of the focusing on the people right because it is you know socio technical with data mesh people are like they just want to talk about the technology and you haven't said a single word about the technology and that's awesome.
I really appreciate that. Also think about how in a let's say in a data warehouse environment how how the same process is driven.
Typically there is a technician that is mapping all the source system is not talking with the business stakeholder is mapping the tables in the source system is modeling them rationalizing them obviously losing the semantic flavor of the data because when you rationalize too much the data you're losing semantic.
In the end maybe you have a 50 page of ear wind schema or no flex schema or I don't know how many other way to model the data you have no and you present this on to the business and they can even not understand what you are presenting
you know there is an information gap between the representation of a data warehouse and what a business stakeholder can understand you know so it's a it's a passive communication from the technical guy to the stakeholder and stakeholder
so okay but then I will have my data I can play with that yeah yeah sure don't worry and then they will discover in the end that okay to consume such data they have to join 10 tables and they don't know which are the join the join semantics to apply not to extract the business concept.
So instead if you start from from the end from the business concept from what you want to achieve it's far easier and also in the way you present to the business stakeholder the the output of this work is very easy to understand for for them so they are really following you know.
So it's you might not want to call it this because you are a technologist but what you're doing is data product marketing right it is the product marketing and it's the product management but it's even kind of the pre product management side of product management of like.
Going and querying your people and that your potential consumers and I like that and kind of what you talk about with the data warehouse as well of you know can I use this if I'm the domain or have I dropped all of my semantics in my context on the floor I'm trying to talk about data mesh a little bit about.
It's attempting to doesn't mean that you wave a magic wand and it happens but that you maximize the context of the data that you're sharing and you maximize the usability right data warehouse has always been about maximizing the usability.
Because it's you know it's very very structured so if you want that structured query that has been.
What have you got or do cross domain things and do all of that so that this is really I love this approach and I'm just even thinking about how you could talk to a team about approaching this I see this as.
Oh I could see people's eyes starting to light up about it so yeah and I also let me add that when you are closing this informative gap between the data engineer or the data model and and the business stakeholder.
The first step towards across functional team know because you are able to work in the same team if you are talking the same language if you understand each other otherwise you can be also in the same team but with totally different languages total different expectation and.
In the end that that that's one is not across functional team is customer supply relationships so yeah it's put it as put it on my jira backlog is what the.
Or put together a confluence page about it is what the data engineering team says and that like we'll get to it when we get to it and it's not that they can fully.
Easily communicate what that is without an actual conversation right that conversation that back and forth is where you fill in those those contextual gaps and that we're not dropping context on the floor where we say hey.
I'm the business person somebody needs this data I just need you to create a pipeline and push this data to them and so the data engineer doesn't necessarily know the context of the business or the context of the consumer and so.
You know I like this and I'd be interested to think through this about how often this gets redone right that this isn't just a one time thing but like how often.
I know it's early enough in your implementations even though it's been almost two years that you probably haven't had to redo these constantly but like how.
Product management is about keeping the product fresh and keeping the features.
Relevant to the consumers so it's an interesting concept i'm not asking if if you've got answers awesome but i'm not expecting anybody to have answers on that for how to do this with data mesh for everybody for.
Now a year or two before we really start to figure that out yeah yeah this is mainly for the inception not to start to draft the domain and all the data products when when or when the domain in the analytical spaces is not exist.
Or when you have to do a transition from from a different paradigm like that's like or data warehouse because if you have a data warehouse.
You maybe you even don't know which are the domains and you don't have an idea of which are the data products now so you have to start from scratch to identify domains and data products and then you understand how to perform the transition from the two practices no because it's really an inversion of.
Of control in the process also in the change of management process or.
Yeah totally totally different story fascinating and and you know when you're talking about kind of existing data products a lot of companies are thinking of those kind of data assets which I actually think is a good phrase for them because.
Their assets depreciate right like they lose value if you're not constantly.
Kind of maintaining them and then combining them into a product I think is is interesting and giving them the capabilities to manage it as a product so.
And this is I really like this process so once you're you've kind of got this setup of what the.
What data products you think are needed whether it's consumer aligned or you know source aligned, you know producer align.
Like what what then is kind of next steps or what what have you found and and also if you could talk a little bit about.
How quickly you're doing this across the business are you trying to do this with every domain upfront are you trying to do this you know domain by domain.
Basically in I start from the end typically in the data mesh adoption you start from one pilot domain so you apply this to such domain and maybe in a specific domain you have several sub domain so you repeat this worship in several sub domain for example I don't know in the HR.
Domain maybe people development recruiting training so there are different.
Different sub domains that you can map with this process.
But obviously you try to create some some segregation at domain and sub domain level to keep the worship duration manageable.
I was just thinking this could be a multi week type of thing versus yeah.
Yeah, yeah, yeah, something that could.
Also because then after we finish with the canvas we formalize everything with.
With the banded context canvas that is a way to represent.
We align it with the data product semantics so you have the input ports the output ports the full list of input and output and so we are creating.
The design for for the implementation for the data product team.
That's awesome and so.
Once you have that implementation plan you know what what what is next steps from that it's is it just kind of building it or and you know I don't want to get too deep into what you're doing from the platform side at least on this one I probably will want to have you back to talk about what you're doing on platform stuff but.
Okay, we've got alignment on what we think we need you know maybe yes every single domains very different but like if you can talk about how many data products you're seeing per domain or relative to size of domain or things like that like what what.
The timeline as to how long it takes to go from implementation plan to actually you know V zero dot one to V one yeah right a lot of questions I do that.
Probably to address the the platform capabilities we need to have another.
If we assume that we have a platformer with all the key enabler in place so with all the abstraction for provisioning.
All the templates to build the data products so all the blueprints not to be the data products in a in an easy and repeatable repeatable way.
Then the implementation of the data product is totally up to the data product team so we we provide the dis this canvas to the data product team.
Obviously we has a consulting firm we maybe we augment the team of the data product owner.
The implementation time for for a single data product if you have all the templates ready also of service ability data quality so.
Everything must be in place and this is the biggest the biggest part of the data mesh journey.
Right now we we we see that for simple data product is something that is today three days something like this and so it's it's quite fast process.
Obviously if instead you have to put I don't know you have to you have a machine learning model no inside your data product that is calling a huge data set and creating propensity models or stuff like this obviously it will take the same time that.
Is taking now to to beat the same the same business logic in another context.
But the data product implementation once you have automated the everything with the platform capabilities is very straightforward in my opinion.
Very interesting and you know once you have the product out there.
What are you doing to kind of think about life cycle and what are you doing to to measure success of that data product you know i've seen some people talk about oh if this data product is.
Is reused that's that's a sign versus you know number of things that of times people access it you know sometimes there's even value in people knowing that they can get to this data so they stop creating shadow copies and it's not that they actually need the data very often but it's it's like you know reduction in shadow copies which is impossible to measure but it's it's a thing.
So how do you think about measuring success and and you know it's a product right so it's got a life cycle how do you think about that that likes life cycle management.
Typically we we we set up net promoter score on on on that a product so.
Also to to to let the data products be trust trustworthy and this is the first metric of of success no no complaints is good.
Typically people data consumers are always complaining about the data that find the they are consuming the second metric is the number of downstream data products so how many.
I prefer to to to calculate how many downstream data products they have instead of how many data consumption because data consumption can happen also for data discovery or just for curiosity so.
Not really meaningful in in the value chain of of the data instead that if someone else is building data products on top of your data product this this is a very good.
A sign of a value and more or less are this this to so we we we prefer to provide value to data product dependencies instead of data consumption.
I was having this conversation with I think Tristan Baker into it and you know we were trying to come up with a model and I went oh crazy was like oh okay so.
We don't charge anybody this for a single consumption because that's data discovery but then if somebody only consumes from it like two to three times then that gets charged at like three X the rate to the data product.
I think there's value here but it's not in a valuable state and then beyond that we we charge it to the data consumer because you know and that we we give credits back to the data producer because of and it was like.
Let's get our bearings straight let's get let's get where we can understand is this valuable before we start getting into super super costing models and super ROI models versus like.
But we've got some data and we just make a data informed decision rather than trying to automate all the data to tell us what is a good data product and what isn't you know.
You're creating you know financial services my my account for financial services a very very very very crucial service to me is when it generates my yearly tax document but it does that once so if it's trying to to measure our consumers engaging with this product and you know and downloading their tax documents multiple times like that's a terrible.
For that specific one so like understanding that it's that exactly what you talked about of the informed decision versus the automated decision versus.
But it's good that you're trying to to be the building model because I'm also trying failing obviously as you said.
But I'm really convinced that paper use or pay as you consume a model should be in place in a model like the data mesh.
Because this can incentivate to enhance the quality also you talk at the several times about the maintenance and know the long term ownership but how can you have a long term ownership if you don't have incomes from from from the data product so.
I'm really looking forward to address this point but at this stage I'm failing like you so yeah I think it's I think it's a thing where as a community you know I've talked about this a little bit on previous episodes like mesh musings and stuff which.
You know call to action is let's have discussions because exactly what you just said is it's totally okay that we don't have answers for everything and the more that we can share information and especially.
You know anti patterns what did you try that didn't work like that's that type of stuff I want to prevent people from going down bad paths right so what why did you try to go down this path what were you trying to achieve and what do you think was the blocker.
Or what why do you think it didn't work and you know is this is something that people should try again it's a great question you know not just on this but on.
Instead of saying what is it that a product what is it an output port I just said what what is not.
I'm making a clarity exactly it's it's kind of you know the term data product in the industry is generally a product that is heavily backed by data and analytics right and so.
You know within data mesh data product is you know these read only products made of data right and so even just that term and analogy of you know how do you differentiate between those two things is interesting stuff.
But if you've got anything else that you wanted to to share about kind of the product flow would love to hear it but this has been super super useful and I found it really interesting and fascinating I want to.
I'll think about this more and I'll probably send you about 40 slack messages next month.
That that that's good but it's really hard to explain without practicing you know because it's a it's a workshop in the end so you need to see the the question that we ask to the stakeholder business.
And then how how we cut the the boundaries of the that's a product since but it's so so soft that's really hard to talk about that.
It's new and but I think you did a great job so you know with that would love to give you a little bit of time to to say you know how people can get in touch with you and if they if they want to have one of these workshops themselves and potentially work with with agile labs.
What's the best way to to get in touch with you or to find you.
The best way to to reach me is on LinkedIn probably so feel feel feel feel free to connect to me and writing in direct and obviously as a agile lab we are we are providing several services around data mesh so.
Data mesh inception workshop or data data data data product flow to identify domains and so on or data mesh strategy so it's.
It's a journey on discovering the readiness of an organization to adopt the data mesh and then we design the roadmap to accomplish with that and then obviously the implementation of of data mesh platforms for our customers and in the end in the next future.
Also, a product around that mesh we'll we've come from from us.
Awesome well thank you very much.
I want to thank my guest today again that was Paolo Plater really enjoyed that conversation learned a lot from it.
If you'd like to contact Paolo his information is in the show notes so please feel free to reach out both his email and his LinkedIn profile link are in the show notes so.
If you think he could be of help to you in some way please reach out to him whether that's to discuss what we covered today or for any other reason he's been a great member of the community since the beginnings.
This has been an episode of data mesh radio hosted by Scott relevant like to thank again data stacks for providing this as a free community resource please stick around after the outro music for a bit of a word from data stacks.
Thank you.
Data mesh radio is a free community resource provided by data stacks if you're not aware I'm actually employed by by data stacks and they've allowed me to be a resource for the the channel data community learning about data mesh data as a product and other related topics.
Data stacks is made a lot of pretty big interesting changes in the last few years and now offer a multi region very performant serverless database offering with a number of easy to develop for API's with like grpc json graph ql rest.
If you'd like to learn more check them out you can check out their astro dp offering and you can use the code dap 500 that's d a a p 500 or data as a product 500 to get $500 of free credits you can also check out what they're doing with streaming it's pretty interesting.

**OPEN CODING TRACE:**
- <ins>Option: </ins>
- <ins>Decision: </ins>
- <ins>Context: </ins>
- <ins>Comments: </ins>

**AXIAL CODING TRACE:**
added:
``` python
#added classes: 
#added relations: 
#added links: 
``` 
add codes to s: 
``` python 
    '''
    '''
# already added: 
```
```python
add_links({s30: [api_as_contract_decision, api_contract_specified, api_contract_specified_first,
                 api_code_first, api_stability, api_modifiability,
                 separation_of_api_contract_and_domain_concerns, initial_effort_required,
                 design_and_implementation_effort, maintainability_of_api_and_consumers, api_understandability
                 ]}, role_name="contained_code")
```
