# s 
55
## url
https://towardsdatascience.com/data-mesh-architecture-patterns-98cc1014f251
## tiny url
https://tinyurl.com/data-as-a-product-s55
## archive url
https://bit.ly/data-as-a-product-s55
## title
Data Mesh Architecture Patterns
## source code
no
## example
yes
## source type 
Practitioner Audience Article
## author type
Practitioner
## references

**AXIAL CODING TRACE**: source and references
``` python
s55 = CClass(source, "s55", values={
    "title": "Data Mesh Architecture Patterns",
    "url": "https://towardsdatascience.com/data-mesh-architecture-patterns-98cc1014f251",
    "archive url": "https://bit.ly/data-as-a-product-s55",
    "tiny url": "https://tinyurl.com/data-as-a-product-s55",
    "author type": "Practitioner",
    "type": "Practitioner Audience Article",
    "example": True,
    "source code": False})
``` 

# coding

> Data Mesh Architecture Patterns

> The enterprise data mesh is revolutionizing how enterprises are managing their data. What are the foundational Data Mesh patterns?

![Architectures](https://miro.medium.com/max/786/1*OV04WPDV_Ovvh3IbcOXjBg.webp)

> Data Mesh Patterns
The enterprise data mesh is emerging as a unique and compelling way to manage data within an enterprise. It brings “product thinking” to enterprise data management while enabling new levels agility and data governance in the enterprise. And it creates a “self-serve” capability with near real-time synchronization of data thereby creating the foundation for real-time digital enterprises.
But, alas, there is no single product that brings you the Data Mesh. Rather, an Enterprise’s Data Mesh is composed of many commonly available components (see next section for a Data Mesh architecture recap).
Still, the key to success is understanding how these components interact. In this article, I will describe these interactions using Architecture Patterns.

> Data Mesh Architecture Recap
> An Enterprise Data Mesh is composed of many components (lots more detail available here, here, and here). Data Products, the primary building block within a Data Mesh, contain operational, analytic, and/or engagement data which is synchronized across the organiation using an Enterprise’s Data Mesh. APIs are used to access data within a Data Product. To support federated governance, each Data Product contains an audit log that records data changes and a catalog of data it manages.
An Enterprise’s Data Mesh has many Data Products. Data Products subscribe to each other’s data such that when one Data Product changes its data, this change is communicated to other Data Products using Change Data Capture and an Event Streaming Backbone.
Lastly, an Enterprise Data Catalog — a synchronized aggregation of all Data Product catalogs and data changes– is used to make it easy for any user or developer to find, consume, and govern any data across the enterprise, while also providing the foundation for understanding data lineage across the enterprise.

![Enterprise](https://miro.medium.com/max/786/1*Px-3vQ2Gy6nq3ijPECT_tw.webp)

> We will describe the following Architecture Patterns in this article:
Change Data Capture
Event Streaming Backbone
Data Product Catalog
Enterprise Data Product Catalog
Immutable Change / Audit Log

**OPEN CODING TRACE:**

Recognised options: change_data_capture, event_streaming, central_data_product_catalogue, data_catalogue, immutable_change_audit_log

**AXIAL CODING TRACE:**

add codes to s55: 
``` python 
    '''
        change_data_capture, 
        event_streaming, 
        central_data_product_catalogue, 
        data_catalogue, 
        immutable_change_audit_log
    '''
```


> Data Mesh Pattern: Change Data Capture
Today, it is difficult to deliver data safely, reliably, and consistently between service and application boundaries. There are two approaches to address this challenge. First, data can be updated synchronously across multiple databases using protocols such as “two-phase commit” (2PC), but this approach is usually complex and costly and is usually reserved for situations where it is absolutely crucial to keep multiple data sources in sync.
A second approach is to update a primary database immediately while a secondary database is updated at a future time (but out of the scope of a transaction). Problems arise when the time span between when the primary and secondary databases are updated is longer than desired.
Change Data Capture (CDC) is a foundational component used by an Enterprise Data Mesh to address this challenge. CDC works by capturing and publishing entries in a database’s transaction log but most importantly it does this unobtrusively and outside of the original transaction. This means that CDC transparently captures changes in operational (or analytic) data without impact to the originating application or transaction flow.
(Note: There is a lot more detail here for those looking for a more detail on how CDC works in an Enterprise)

![ChangeDataCapture](https://miro.medium.com/max/786/1*uYKTxPrFlP27M6WIFAA9UA.webp)

> But what does CDC do with the captured “event”. In the Enterprise Data Mesh, it publishes the event to an Event Streaming Backbone (the next pattern) for distribution across the enterprise.

> Data Mesh Pattern: Event Streaming Backbone
An Event Streaming Backbone distributes events in an Enterprise Data Mesh. Events commonly originate from applications, APIs, and, in our case, also from CDC. However, of particular importance is that any published event can be safely, reliably, and in near real-time consumed by any other subscribing entity.

![EventStreamingBackbone](https://miro.medium.com/max/786/1*I3MqqFN8ZNvmlzC4AIBQWg.webp)

> There are several core managed entities in an Event Streaming Backbone:
Events, defined by JSON Schemas, are distributed within the Enterprise Data Mesh.
Topics are used to queue and distribute events across the enterprise; The Enterprise Data Mesh uses well-known topics that act like queues by allowing many entities to publish and consume events.
Producers publish events to topics; Producers in an Enterprise Data Mesh may be APIs, applications, or CDC.
Consumers consume events from topics. Consumers in an Enterprise Data Mesh may be any entity or application that subscribe to a topic and are notified when an event is available for processing.
Event Stream Processors process events either on a per-event or aggregating on a time-window basis enabling for very sophisticated and powerful analysis techniques in an Enterprise Data Mesh.
Brokers manage the above components to ensure safe and reliable event communications across the entire Enterprise Data Mesh.

**OPEN CODING TRACE:**

Recognised options: real_time_data_access

**AXIAL CODING TRACE:**

add codes to s55: 
``` python 
    '''
        real_time_data_access
    '''
```

> Data Mesh Pattern: Data Product Catalogs
Data, they say, is the new gold and mining it will bring immense insight and wealth. But in most enterprises today data is strewn across many groups in an organization. Sales owns customer data, distribution owns supply chain, and finance owns transactions and accounts.
Unfortunately, this makes it very difficult to find data, and once found, it is even more difficult to bring it together to make holistic business decisions. The result: slow, costly, and uninformed decision making.
A Data Product Catalog (DPC) contains information about data (“meta-data”) for a Data Product. The DPC provides the information that makes it easy for any authorized person or application to find, view, and consume Data Products in the Enterprise Data Mesh. A DPC offers several benefits:
Easy to Manage, by enabling local ownership and accountability.
Easy to Change and Grow, by allowing decision making to be localized and faster.
Easy to Find, View, and Consume data, by making by making it easy for any (authorized) entity to find, view, and consume data (i.e., “self-serve”).

![DataProductCatalog](https://miro.medium.com/max/786/1*30LknC0f4JipYpNQ-NMN1A.webp)

**OPEN CODING TRACE:**

Recognised options: delegated_ownership, consumption, discoverability, up_to_date

**AXIAL CODING TRACE:**

add codes to s55: 
``` python 
    add_force_relations({data_catalogue: {standardised_transformation: positive,
                                          duplication: negative,
                                          obscurity: negative,
                                          discoverability: positive,
                                          data_search: positive,
                                          data_enrichment: positive,
                                          delegated_ownership: positive, 
                                          consumption: very_positive,
                                          discoverability: very_positive,
                                          up_to_date: positive}
                       })
```

> Data Mesh Pattern: Enterprise Data Product Catalog
Enterprise Data Product Catalog (EDPC) is a repository that aggregates meta-data from all local Data Product Catalogs (DPCs). The Enterprise Data Catalog is used to store information and statistics — meta-data — about all data maintained in an Enterprise Data Mesh making it easy to find, view, consume, and govern data:
Data Scientists use the EDPC to find the location of data in the enterprise that they can use to train their models.
Business Users use the EDPC to find information required for business decisions.
Developers use the EDPC to understand data structures required by their applications.
Governance Professionals uses EDPC to understand and monitor data across the enterprise, enabling federated computational governance within the Enterprise Data Mesh.

![EnterpriseCatalogue](https://miro.medium.com/max/786/1*Ga4kYPXvxzh_Z0EoZJ72gQ.webp)

**OPEN CODING TRACE:**

Recognised forces: discoverability, understandability, observability

**AXIAL CODING TRACE:**

add codes to s55: 
``` python 
    add_force_relations({central_data_product_catalogue: {discoverability: positive,
                                                    understandability: positive, 
                                                     observability: positive}
                       })
```

> Data Mesh Pattern: Immutable Change/Audit Log
Understanding the lineage of data — defined as the aggregated list of changes data undergoes — is crucial for governance and regulatory purpose. Why is this important? Consider a common situation today: The emergence of AI/Machine Learning is now a mandatory enterprise capability. Data Scientists use sophisticated models to enable and make critical business decisions.
However, in many enterprises, especially those in health care and finance, the actual viability of these models is predicated on the ability to meet regulators demands for reproducibility and traceability (more information is available here and here). Unfortunately, most enterprise do not have the capability to track data lineage in a fashion required by auditors or regulators.
An Enterprise Data Mesh’s Immutable Change/Audit Log addresses this need by retaining a history data changes within the Enterprise Data Mesh for future audit and governance purposes. Local Data Product change/audit logs are updated automatically upon any data change in the data. These logs are then propagated to an Enterprise Data Product Catalog (EDPC) allowing for a consolidated history of data changes in the enterprise.
In other words, the EDPC contains the data lineage for all elements within the Enterprise Data Mesh. The EDPC uses this data to provide a searchable index of meta-data — which explicitly includes references to each Data Product’s immutable change/audit log — allows data lineage to be easily found and confirmed.

![ImmutableChangeAuditLog](https://miro.medium.com/max/1400/1*smRauzl58Bkvr_wz-gIyNw.webp)

**OPEN CODING TRACE:**

Recognised forces: reproducibility, traceability, data_lineage
New force: governance

**AXIAL CODING TRACE:**

add codes to s55: 
``` python 
    governance = CClass(force, "Governance")
    
    add_force_relations({immutable_change_audit_log: {reproducibility: positive,
                                                traceability: positive,
                                                governance: very_positive,
                                                data_lineage: positive}
                       })        
```

> Concluding Thoughts
An Enterprise Data Mesh is becoming a foundational enabler of the real-time digital enterprise. Architecture patterns provide an established way to describe Data Mesh interactions. And while there is no readily available tool available, the first step to building your own organizations Data Mesh is to understand the foundational patterns that enable a Data Mesh.
Hopefully this article gives you the necessary insight to kickstart your own Enterprise Data Mesh!